{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Running setup.py install for pyspark: started\n",
      "  Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Onur\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-UTIUBO9M:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FordGoBike</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1faaa901220>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "spark=SparkSession.builder.appName('FordGoBike').getOrCreate()\n",
    "spark\n",
    "# https://www.kaggle.com/code/priyankabnl/fordgobike-trip-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"G:/My Drive/Develhope/develhope-Data5-Team3/Week_1/2017-fordgobike-tripdataa.csv\"\n",
    "df_pyspark=spark.read.csv(path,header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(start_time='57:39.7', start time hour=4, start time minute=57, start time seconds=40, _c4='PM', end_time='12:50.2', end_time hour=3, end_time minute=12, end_time seconds=50, _c9='PM', start_station_id=74, start_station_name='Laguna St at Hayes St', start_station_latitude=37.77643482, start_station_longitude=-122.426244, end_station_id=43, end_station_name='San Francisco Public Library (Grove St at Hyde St)', end_station_latitude=37.7787677, end_station_longitude=-122.4159292, bike_id=96, user_type='Customer', member_birth_year=1987, member_gender='Male', pyment='credit card'),\n",
       " Row(start_time='56:34.8', start time hour=3, start time minute=56, start time seconds=35, _c4='PM', end_time='49:55.6', end_time hour=1, end_time minute=49, end_time seconds=56, _c9='PM', start_station_id=284, start_station_name='Yerba Buena Center for the Arts (Howard St at 3rd St)', start_station_latitude=37.78487208, start_station_longitude=-122.4008757, end_station_id=96, end_station_name='Dolores St at 15th St', end_station_latitude=37.7662102, end_station_longitude=-122.4266136, bike_id=88, user_type='Customer', member_birth_year=1965, member_gender='Female', pyment='credit card'),\n",
       " Row(start_time='45:48.4', start time hour=10, start time minute=45, start time seconds=48, _c4='PM', end_time='28:36.9', end_time hour=11, end_time minute=28, end_time seconds=37, _c9='AM', start_station_id=245, start_station_name='Downtown Berkeley BART', start_station_latitude=37.8703477, start_station_longitude=-122.2677637, end_station_id=245, end_station_name='Downtown Berkeley BART', end_station_latitude=37.8703477, end_station_longitude=-122.2677637, bike_id=1094, user_type='Customer', member_birth_year=None, member_gender=None, pyment='credit card'),\n",
       " Row(start_time='31:10.6', start time hour=5, start time minute=31, start time seconds=11, _c4='PM', end_time='47:23.5', end_time hour=10, end_time minute=47, end_time seconds=24, _c9='AM', start_station_id=60, start_station_name='8th St at Ringold St', start_station_latitude=37.7745204, start_station_longitude=-122.4094494, end_station_id=5, end_station_name='Powell St BART Station (Market St at 5th St)', end_station_latitude=37.78389936, end_station_longitude=-122.4084449, bike_id=2831, user_type='Customer', member_birth_year=None, member_gender=None, pyment='credit card'),\n",
       " Row(start_time='23:14.0', start time hour=2, start time minute=23, start time seconds=14, _c4='PM', end_time='29:57.6', end_time hour=2, end_time minute=29, end_time seconds=58, _c9='AM', start_station_id=239, start_station_name='Bancroft Way at Telegraph Ave', start_station_latitude=37.8688126, start_station_longitude=-122.258764, end_station_id=247, end_station_name='Fulton St at Bancroft Way', end_station_latitude=37.8677892, end_station_longitude=-122.2658964, bike_id=3167, user_type='Subscriber', member_birth_year=1997, member_gender='Female', pyment='app wallet')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- start time hour: integer (nullable = true)\n",
      " |-- start time minute: integer (nullable = true)\n",
      " |-- start time seconds: integer (nullable = true)\n",
      " |-- start_am_pm: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- end_time hour: integer (nullable = true)\n",
      " |-- end_time minute: integer (nullable = true)\n",
      " |-- end_time seconds: integer (nullable = true)\n",
      " |-- end_am_pm: string (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: float (nullable = true)\n",
      " |-- start_station_longitude: float (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: float (nullable = true)\n",
      " |-- end_station_longitude: float (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- member_birth_year: integer (nullable = true)\n",
      " |-- member_gender: string (nullable = true)\n",
      " |-- pyment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=df_pyspark.withColumn('start_station_longitude',F.col('start_station_longitude').cast(FloatType()))\\\n",
    "    .withColumn('start_station_latitude',F.col('start_station_latitude').cast(FloatType()))\\\n",
    "    .withColumn('end_station_latitude',F.col('end_station_latitude').cast(FloatType()))\\\n",
    "    .withColumn('end_station_longitude',F.col('end_station_longitude').cast(FloatType()))\n",
    "df_pyspark=df_pyspark.withColumnRenamed('_c4','start_am_pm').withColumnRenamed('_c9','end_am_pm')\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+------------------+------------------+-----------+--------+-----------------+------------------+------------------+---------+-----------------+--------------------+----------------------+-----------------------+-----------------+--------------------+--------------------+---------------------+------------------+----------+------------------+-------------+-----------+\n",
      "|summary|start_time|  start time hour| start time minute|start time seconds|start_am_pm|end_time|    end_time hour|   end_time minute|  end_time seconds|end_am_pm| start_station_id|  start_station_name|start_station_latitude|start_station_longitude|   end_station_id|    end_station_name|end_station_latitude|end_station_longitude|           bike_id| user_type| member_birth_year|member_gender|     pyment|\n",
      "+-------+----------+-----------------+------------------+------------------+-----------+--------+-----------------+------------------+------------------+---------+-----------------+--------------------+----------------------+-----------------------+-----------------+--------------------+--------------------+---------------------+------------------+----------+------------------+-------------+-----------+\n",
      "|  count|    519700|           519700|            519700|            519700|     519700|  519700|           519700|            519700|            519700|   519700|           519700|              519700|                519700|                 519700|           519700|              519700|              519700|               519700|            519700|    519700|            453159|       453238|     519700|\n",
      "|   mean|      null|6.678974408312488|29.297106022705407| 29.54025591687512|       null|    null|6.708999422743891| 29.45680200115451|29.601156436405617|     null|95.03424475659034|                null|     37.77165276342983|    -122.36392742441048|92.18404079276506|                null|   37.77184392299656|  -122.36323564931294|1672.5330786992495|      null|1980.4047872821682|         null|       null|\n",
      "| stddev|      null|2.971605372698218|17.376531263068017| 17.33122774147236|       null|    null|2.962465107146883|17.340763809250042|17.300185488488726|     null|86.08307797095989|                null|   0.08630505434142516|    0.10557361044333616|84.96949148635471|                null|  0.0862239211762343|  0.10512237725597676| 971.3569593530091|      null|10.513487539908484|         null|       null|\n",
      "|    min|   00:00.0|                1|                 0|                 0|         AM| 00:00.0|                1|                 0|                 0|       AM|                3|10th Ave at E 15t...|               37.3173|             -122.44429|                3|10th Ave at E 15t...|             37.3173|           -122.44429|                10|  Customer|              1886|       Female| app wallet|\n",
      "|    max|   59:59.9|               12|                59|                59|         PM| 59:59.9|               12|                59|                59|       PM|              340|Yerba Buena Cente...|             37.880222|            -121.874115|              340|Yerba Buena Cente...|           37.880222|          -121.874115|              3733|Subscriber|              1999|        Other|credit card|\n",
      "+-------+----------+-----------------+------------------+------------------+-----------+--------+-----------------+------------------+------------------+---------+-----------------+--------------------+----------------------+-----------------------+-----------------+--------------------+--------------------+---------------------+------------------+----------+------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark= df_pyspark.withColumn('start time hour',F.when(df_pyspark['start_am_pm']=='PM',df_pyspark[\"start time hour\"]+12).otherwise(df_pyspark[\"start time hour\"]))\n",
    "df_pyspark= df_pyspark.withColumn('start time hour',F.when(df_pyspark['start time hour']==24,12).otherwise(df_pyspark[\"start time hour\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark= df_pyspark.withColumn('end_time hour',F.when(df_pyspark['end_am_pm']=='PM',df_pyspark[\"end_time hour\"]+12).otherwise(df_pyspark[\"end_time hour\"]))\n",
    "df_pyspark= df_pyspark.withColumn('end_time hour',F.when(df_pyspark['end_time hour']==24,12).otherwise(df_pyspark[\"end_time hour\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(F.col('end_time hour')==24).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing 1-digit values \n",
    "df_pyspark=df_pyspark.withColumn('start time hour',F.lpad(F.col('start time hour'),2,'0'))\\\n",
    "    .withColumn('start time minute',F.lpad(F.col('start time minute'),2,'0'))\\\n",
    "    .withColumn('start time seconds',F.lpad(F.col('start time seconds'),2,'0'))\\\n",
    "    .withColumn('end_time hour',F.lpad(F.col('end_time hour'),2,'0'))\\\n",
    "    .withColumn('end_time minute',F.lpad(F.col('end_time minute'),2,'0'))\\\n",
    "    .withColumn('end_time seconds',F.lpad(F.col('end_time seconds'),2,'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating start time and end time columns with timestamp for easy comparison\n",
    "df_pyspark=df_pyspark.withColumn('start time',F.to_timestamp(F.concat_ws(':',F.lpad(F.col('start time hour'),2,'0'),F.lpad(F.col('start time minute'),2,'0'),F.lpad(F.col('start time seconds'),2,'0'))))\n",
    "df_pyspark=df_pyspark.withColumn('end time',F.to_timestamp(F.concat_ws(':',F.lpad(F.col('end_time hour'),2,'0'),F.lpad(F.col('end_time minute'),2,'0'),F.lpad(F.col('end_time seconds'),2,'0'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping end time and start time values if start time > end time\n",
    "df_pyspark=df_pyspark.withColumn('end time',F.when(df_pyspark['end time']<df_pyspark['start time'], df_pyspark['start time'])\\\n",
    "                                 .otherwise(df_pyspark['end time']))\n",
    "df_pyspark=df_pyspark.withColumn('start time',F.when(df_pyspark['end time']==df_pyspark['start time'], \n",
    "                                                     F.to_timestamp(F.concat_ws(':','end_time hour','end_time minute','end_time seconds')))\\\n",
    "                                 .otherwise(df_pyspark['start time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'lapd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pyspark\u001b[39m=\u001b[39mdf_pyspark\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mstart_time\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39mconcat_ws(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39mlpad(F\u001b[39m.\u001b[39mhour(F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mstart time\u001b[39m\u001b[39m'\u001b[39m)),\u001b[39m2\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m),F\u001b[39m.\u001b[39;49mlapd(F\u001b[39m.\u001b[39mminute(F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mstart time\u001b[39m\u001b[39m'\u001b[39m)),\u001b[39m2\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m),F\u001b[39m.\u001b[39mlapd(F\u001b[39m.\u001b[39msecond(F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mstart time\u001b[39m\u001b[39m'\u001b[39m)),\u001b[39m2\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m)))\u001b[39m#\\\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[39m#.withColumn('end_time',F.concat_ws(':',F.lpad(F.hour('end time'),2,'0'),F.lpad(F.minute('end time'),2,'0'),F.lpad(F.second('end time'),2,'0')))\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'lapd'"
     ]
    }
   ],
   "source": [
    "# df_pyspark=df_pyspark.withColumn('start_time',F.concat_ws(':',F.lpad(F.hour(F.col('start time')),2,'0'),F.lapd(F.minute(F.col('start time')),2,'0'),F.lapd(F.second(F.col('start time')),2,'0')))#\\\n",
    "    #.withColumn('end_time',F.concat_ws(':',F.lpad(F.hour('end time'),2,'0'),F.lpad(F.minute('end time'),2,'0'),F.lpad(F.second('end time'),2,'0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.withColumn('start_time',df_pyspark['start time']).withColumn('end_time',df_pyspark['end time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start_time',\n",
       " 'start time hour',\n",
       " 'start time minute',\n",
       " 'start time seconds',\n",
       " 'start_am_pm',\n",
       " 'end_time',\n",
       " 'end_time hour',\n",
       " 'end_time minute',\n",
       " 'end_time seconds',\n",
       " 'end_am_pm',\n",
       " 'start_station_id',\n",
       " 'start_station_name',\n",
       " 'start_station_latitude',\n",
       " 'start_station_longitude',\n",
       " 'end_station_id',\n",
       " 'end_station_name',\n",
       " 'end_station_latitude',\n",
       " 'end_station_longitude',\n",
       " 'bike_id',\n",
       " 'user_type',\n",
       " 'member_birth_year',\n",
       " 'member_gender',\n",
       " 'pyment',\n",
       " 'start time',\n",
       " 'end time']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------------+---------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+-------+----------+-----------------+-------------+-----------+\n",
      "|         start_time|start_am_pm|           end_time|end_am_pm|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude|bike_id| user_type|member_birth_year|member_gender|     pyment|\n",
      "+-------------------+-----------+-------------------+---------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+-------+----------+-----------------+-------------+-----------+\n",
      "|2023-05-07 15:12:50|         PM|2023-05-07 16:57:40|       PM|              74|Laguna St at Haye...|             37.776436|             -122.42625|            43|San Francisco Pub...|           37.778767|           -122.41593|     96|  Customer|             1987|         Male|credit card|\n",
      "|2023-05-07 13:49:56|         PM|2023-05-07 15:56:35|       PM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|            96|Dolores St at 15t...|            37.76621|           -122.42661|     88|  Customer|             1965|       Female|credit card|\n",
      "|2023-05-07 11:28:37|         PM|2023-05-07 22:45:48|       AM|             245|Downtown Berkeley...|             37.870346|             -122.26776|           245|Downtown Berkeley...|           37.870346|           -122.26776|   1094|  Customer|             null|         null|credit card|\n",
      "|2023-05-07 10:47:24|         PM|2023-05-07 17:31:11|       AM|              60|8th St at Ringold St|              37.77452|             -122.40945|             5|Powell St BART St...|             37.7839|           -122.40845|   2831|  Customer|             null|         null|credit card|\n",
      "|2023-05-07 02:29:58|         PM|2023-05-07 14:23:14|       AM|             239|Bancroft Way at T...|             37.868813|            -122.258766|           247|Fulton St at Banc...|            37.86779|            -122.2659|   3167|Subscriber|             1997|       Female| app wallet|\n",
      "|2023-05-07 01:24:47|         PM|2023-05-07 22:51:01|       AM|              30|San Francisco Cal...|               37.7766|             -122.39528|            30|San Francisco Cal...|             37.7766|           -122.39528|   1487|  Customer|             null|         null| app wallet|\n",
      "|2023-05-07 01:04:36|         PM|2023-05-07 23:49:28|       AM|             259|Addison St at Fou...|              37.86625|             -122.29937|           259|Addison St at Fou...|            37.86625|           -122.29937|   3539|  Customer|             1991|       Female| app wallet|\n",
      "|2023-05-07 12:58:51|         PM|2023-05-07 23:46:37|       AM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|           284|Yerba Buena Cente...|           37.784874|           -122.40088|   1503|  Customer|             null|         null| app wallet|\n",
      "|2023-05-07 12:46:18|         PM|2023-05-07 23:37:08|       AM|              20|Mechanics Monumen...|               37.7913|             -122.39905|            20|Mechanics Monumen...|             37.7913|           -122.39905|   3125|  Customer|             null|         null| app wallet|\n",
      "|2023-05-07 12:46:17|         PM|2023-05-07 23:35:38|       AM|              20|Mechanics Monumen...|               37.7913|             -122.39905|            20|Mechanics Monumen...|             37.7913|           -122.39905|   2543|  Customer|             null|         null| app wallet|\n",
      "|2023-05-07 12:41:25|         PM|2023-05-07 23:46:32|       AM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|            22|Howard St at Beal...|           37.789757|          -122.394646|   3058|  Customer|             null|         null|credit card|\n",
      "|2023-05-07 12:41:10|         PM|2023-05-07 23:48:12|       AM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|            22|Howard St at Beal...|           37.789757|          -122.394646|   3197|  Customer|             null|         null| app wallet|\n",
      "|2023-05-07 12:29:19|         PM|2023-05-07 23:52:56|       AM|              67|San Francisco Cal...|             37.776638|             -122.39552|            24|Spear St at Folso...|           37.789677|           -122.39043|   2311|Subscriber|             1990|         Male|credit card|\n",
      "|2023-05-07 12:29:07|         PM|2023-05-07 23:52:56|       AM|              67|San Francisco Cal...|             37.776638|             -122.39552|            24|Spear St at Folso...|           37.789677|           -122.39043|   3717|Subscriber|             1990|         Male| app wallet|\n",
      "|2023-05-07 12:20:21|         PM|2023-05-07 23:35:23|       AM|              66|3rd St at Townsen...|              37.77874|             -122.39274|            23|The Embarcadero a...|           37.791466|           -122.39104|   3452|  Customer|             null|         null|credit card|\n",
      "|2023-05-07 12:19:23|         PM|2023-05-07 23:53:39|       AM|              14|Clay St at Batter...|             37.795002|             -122.39997|            27|Beale St at Harri...|            37.78806|           -122.39187|    558|Subscriber|             1980|       Female|credit card|\n",
      "|2023-05-07 12:19:14|         PM|2023-05-07 23:54:40|       AM|              14|Clay St at Batter...|             37.795002|             -122.39997|            27|Beale St at Harri...|            37.78806|           -122.39187|   3646|Subscriber|             1979|         Male| app wallet|\n",
      "|2023-05-07 12:18:27|         PM|2023-05-07 23:55:10|       AM|              78| Folsom St at 9th St|             37.773716|            -122.411644|            15|San Francisco Fer...|            37.79539|            -122.3942|   1667|  Customer|             null|         null| app wallet|\n",
      "|2023-05-07 12:18:22|         PM|2023-05-07 23:52:49|       AM|              78| Folsom St at 9th St|             37.773716|            -122.411644|            15|San Francisco Fer...|            37.79539|            -122.3942|   3114|Subscriber|             1988|        Other| app wallet|\n",
      "|2023-05-07 12:06:50|         PM|2023-05-07 23:46:34|       AM|               4|Cyril Magnin St a...|              37.78588|             -122.40891|           123|Folsom St at 19th St|           37.760593|           -122.41482|   1473|Subscriber|             1971|         Male|credit card|\n",
      "+-------------------+-----------+-------------------+---------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+-------+----------+-----------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping 'start time' and 'end time' columns\n",
    "df_pyspark=df_pyspark.drop(*['start time','end time','start time hour',\n",
    " 'start time minute',\n",
    " 'start time seconds',\n",
    " 'end_time hour',\n",
    " 'end_time minute',\n",
    " 'end_time seconds',])\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haversine\n",
      "  Using cached haversine-2.8.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Onur\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "\n",
    "def haversine_f(lat1, lon1, lat2, lon2):\n",
    "    return haversine( (lat1, lon1), (lat2, lon2),unit='m',normalize=True )\n",
    "\n",
    "haversine_udf = F.udf(haversine_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate haversine distance(Onur)\n",
    "df_pyspark=df_pyspark.withColumn('haversine_distance', \n",
    "                    haversine_udf(F.col('start_station_latitude'), F.col('start_station_longitude'), \n",
    "                                  F.col('end_station_latitude'), F.col('end_station_longitude'))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate haversine distance in meters(Uros)\n",
    "df_pyspark=df_pyspark.withColumn('haversine_distance',\n",
    "                                 haversine_udf('start_station_latitude', \n",
    "                                               'start_station_longitude', \n",
    "                                               'end_station_latitude', \n",
    "                                               'end_station_longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- start_am_pm: string (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- end_am_pm: string (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: float (nullable = true)\n",
      " |-- start_station_longitude: float (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: float (nullable = true)\n",
      " |-- end_station_longitude: float (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- member_birth_year: integer (nullable = true)\n",
      " |-- member_gender: string (nullable = true)\n",
      " |-- pyment: string (nullable = true)\n",
      " |-- haversine_distance: string (nullable = true)\n",
      " |-- Diff_in_seconds: long (nullable = true)\n",
      " |-- Diff_in_minutes: double (nullable = true)\n",
      " |-- Trip_cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#assign timestamp to start_time and end_time\n",
    "#Calculate 'Diff_in_seconds' \n",
    "#Calculate 'Diff_in_minutes' \n",
    "#Calculate 'Trip_cost' \n",
    "df_pyspark=df_pyspark.withColumn('start_time',F.to_timestamp('start_time','HH:mm:ss'))\\\n",
    "    .withColumn('end_time',F.to_timestamp('end_time','HH:mm:ss'))\\\n",
    "    .withColumn('Diff_in_seconds',F.col('end_time').cast('long')-F.col('start_time').cast('long'))\\\n",
    "    .withColumn('Diff_in_minutes',(F.col('Diff_in_seconds')/60))\\\n",
    "    .withColumn('Trip_cost',(F.col('Diff_in_minutes')*0.35))\n",
    "\n",
    "df_pyspark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[bike_id: int, sum_distance: double]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"bike_id\")\\\n",
    "    .agg(F.sum(\"haversine_distance\").alias(\"sum_distance\")).sort(F.desc(\"sum_distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------------+---------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+-------+----------+-----------------+-------------+-----------+------------------+---------------+------------------+------------------+\n",
      "|         start_time|start_am_pm|           end_time|end_am_pm|start_station_id|  start_station_name|start_station_latitude|start_station_longitude|end_station_id|    end_station_name|end_station_latitude|end_station_longitude|bike_id| user_type|member_birth_year|member_gender|     pyment|haversine_distance|Diff_in_seconds|   Diff_in_minutes|         Trip_cost|\n",
      "+-------------------+-----------+-------------------+---------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+-------+----------+-----------------+-------------+-----------+------------------+---------------+------------------+------------------+\n",
      "|2023-05-07 15:12:50|         PM|2023-05-07 16:57:40|       PM|              74|Laguna St at Haye...|             37.776436|             -122.42625|            43|San Francisco Pub...|           37.778767|           -122.41593|     96|  Customer|             1987|         Male|credit card| 942.8784351097447|           6290|104.83333333333333| 36.69166666666666|\n",
      "|2023-05-07 13:49:56|         PM|2023-05-07 15:56:35|       PM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|            96|Dolores St at 15t...|            37.76621|           -122.42661|     88|  Customer|             1965|       Female|credit card|3069.7323740018182|           7599|            126.65|           44.3275|\n",
      "|2023-05-07 11:28:37|         PM|2023-05-07 22:45:48|       AM|             245|Downtown Berkeley...|             37.870346|             -122.26776|           245|Downtown Berkeley...|           37.870346|           -122.26776|   1094|  Customer|             null|         null|credit card|               0.0|          40631| 677.1833333333333|237.01416666666663|\n",
      "|2023-05-07 10:47:24|         PM|2023-05-07 17:31:11|       AM|              60|8th St at Ringold St|              37.77452|             -122.40945|             5|Powell St BART St...|             37.7839|           -122.40845|   2831|  Customer|             null|         null|credit card|1046.7397148339949|          24227|403.78333333333336|141.32416666666666|\n",
      "|2023-05-07 02:29:58|         PM|2023-05-07 14:23:14|       AM|             239|Bancroft Way at T...|             37.868813|            -122.258766|           247|Fulton St at Banc...|            37.86779|            -122.2659|   3167|Subscriber|             1997|       Female| app wallet| 636.4127814825664|          42796| 713.2666666666667|249.64333333333332|\n",
      "|2023-05-07 01:24:47|         PM|2023-05-07 22:51:01|       AM|              30|San Francisco Cal...|               37.7766|             -122.39528|            30|San Francisco Cal...|             37.7766|           -122.39528|   1487|  Customer|             null|         null| app wallet|               0.0|          77174|1286.2333333333333| 450.1816666666666|\n",
      "|2023-05-07 01:04:36|         PM|2023-05-07 23:49:28|       AM|             259|Addison St at Fou...|              37.86625|             -122.29937|           259|Addison St at Fou...|            37.86625|           -122.29937|   3539|  Customer|             1991|       Female| app wallet|               0.0|          81892|1364.8666666666666|477.70333333333326|\n",
      "|2023-05-07 12:58:51|         PM|2023-05-07 23:46:37|       AM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|           284|Yerba Buena Cente...|           37.784874|           -122.40088|   1503|  Customer|             null|         null| app wallet|               0.0|          38866| 647.7666666666667| 226.7183333333333|\n",
      "|2023-05-07 12:46:18|         PM|2023-05-07 23:37:08|       AM|              20|Mechanics Monumen...|               37.7913|             -122.39905|            20|Mechanics Monumen...|             37.7913|           -122.39905|   3125|  Customer|             null|         null| app wallet|               0.0|          39050| 650.8333333333334|227.79166666666666|\n",
      "|2023-05-07 12:46:17|         PM|2023-05-07 23:35:38|       AM|              20|Mechanics Monumen...|               37.7913|             -122.39905|            20|Mechanics Monumen...|             37.7913|           -122.39905|   2543|  Customer|             null|         null| app wallet|               0.0|          38961|            649.35|227.27249999999998|\n",
      "|2023-05-07 12:41:25|         PM|2023-05-07 23:46:32|       AM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|            22|Howard St at Beal...|           37.789757|          -122.394646|   3058|  Customer|             null|         null|credit card| 771.2470743585559|          39907| 665.1166666666667|232.79083333333332|\n",
      "|2023-05-07 12:41:10|         PM|2023-05-07 23:48:12|       AM|             284|Yerba Buena Cente...|             37.784874|             -122.40088|            22|Howard St at Beal...|           37.789757|          -122.394646|   3197|  Customer|             null|         null| app wallet| 771.2470743585559|          40022| 667.0333333333333|233.46166666666664|\n",
      "|2023-05-07 12:29:19|         PM|2023-05-07 23:52:56|       AM|              67|San Francisco Cal...|             37.776638|             -122.39552|            24|Spear St at Folso...|           37.789677|           -122.39043|   2311|Subscriber|             1990|         Male|credit card|1517.4357297216957|          41017| 683.6166666666667|239.26583333333332|\n",
      "|2023-05-07 12:29:07|         PM|2023-05-07 23:52:56|       AM|              67|San Francisco Cal...|             37.776638|             -122.39552|            24|Spear St at Folso...|           37.789677|           -122.39043|   3717|Subscriber|             1990|         Male| app wallet|1517.4357297216957|          41029| 683.8166666666667|239.33583333333334|\n",
      "|2023-05-07 12:20:21|         PM|2023-05-07 23:35:23|       AM|              66|3rd St at Townsen...|              37.77874|             -122.39274|            23|The Embarcadero a...|           37.791466|           -122.39104|   3452|  Customer|             null|         null|credit card|1422.9265359340616|          40502| 675.0333333333333|236.26166666666663|\n",
      "|2023-05-07 12:19:23|         PM|2023-05-07 23:53:39|       AM|              14|Clay St at Batter...|             37.795002|             -122.39997|            27|Beale St at Harri...|            37.78806|           -122.39187|    558|Subscriber|             1980|       Female|credit card|1050.1837267507703|          41656| 694.2666666666667| 242.9933333333333|\n",
      "|2023-05-07 12:19:14|         PM|2023-05-07 23:54:40|       AM|              14|Clay St at Batter...|             37.795002|             -122.39997|            27|Beale St at Harri...|            37.78806|           -122.39187|   3646|Subscriber|             1979|         Male| app wallet|1050.1837267507703|          41726| 695.4333333333333|243.40166666666664|\n",
      "|2023-05-07 12:18:27|         PM|2023-05-07 23:55:10|       AM|              78| Folsom St at 9th St|             37.773716|            -122.411644|            15|San Francisco Fer...|            37.79539|            -122.3942|   1667|  Customer|             null|         null| app wallet| 2856.228913866177|          41803| 696.7166666666667|243.85083333333333|\n",
      "|2023-05-07 12:18:22|         PM|2023-05-07 23:52:49|       AM|              78| Folsom St at 9th St|             37.773716|            -122.411644|            15|San Francisco Fer...|            37.79539|            -122.3942|   3114|Subscriber|             1988|        Other| app wallet| 2856.228913866177|          41667|            694.45|          243.0575|\n",
      "|2023-05-07 12:06:50|         PM|2023-05-07 23:46:34|       AM|               4|Cyril Magnin St a...|              37.78588|             -122.40891|           123|Folsom St at 19th St|           37.760593|           -122.41482|   1473|Subscriber|             1971|         Male|credit card| 2859.360089540606|          41984| 699.7333333333333|244.90666666666667|\n",
      "+-------------------+-----------+-------------------+---------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+-------+----------+-----------------+-------------+-----------+------------------+---------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1339.csv.\n: ExitCodeException exitCode=1: ChangeFileModeByMask error (87): The parameter is incorrect.\r\n\r\n\r\n\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pyspark\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39mG:/My Drive/Develhope/mycsv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[0;32m   1781\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[0;32m   1782\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m   1783\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1797\u001b[0m     lineSep\u001b[39m=\u001b[39mlineSep,\n\u001b[0;32m   1798\u001b[0m )\n\u001b[1;32m-> 1799\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1339.csv.\n: ExitCodeException exitCode=1: ChangeFileModeByMask error (87): The parameter is incorrect.\r\n\r\n\r\n\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.write.option(\"header\",True).mode('overwrite').csv('G:/My Drive/Develhope/mycsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Onur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pyspark\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\Onur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Onur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Onur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Onur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
